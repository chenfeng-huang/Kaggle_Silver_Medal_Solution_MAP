{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132154a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-index --find-links=/kaggle/input/transformers_package transformers -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen3-8b\n",
    "!torchrun --nproc_per_node=2 inference.py \\\n",
    "  --model_name \"/kaggle/input/qwen-3/transformers/8b/1\" \\\n",
    "  --lora_path \"/kaggle/input/qwen3-8b-models-adaptors\" \\\n",
    "  --output_filename \"submission_qwen3_8b_prob\"\n",
    "\n",
    "# qwen3-14b\n",
    "!torchrun --nproc_per_node=2 inference.py \\\n",
    "    --model_name \"/kaggle/input/qwen-3/transformers/14b/1\" \\\n",
    "    --lora_path \"/kaggle/input/qwen3-14b-models-adaptors\" \\\n",
    "    --output_filename \"submission_qwen3_14b_prob\"\n",
    "\n",
    "# qwen3-4b\n",
    "!torchrun --nproc_per_node=2 inference.py \\\n",
    "    --model_name \"/kaggle/input/qwen-3-4b-instruct-2507\" \\\n",
    "    --lora_path \"/kaggle/input/qwen3-4b-models-adaptors\" \\\n",
    "    --output_filename \"submission_qwen3_4b_prob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa59966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------\n",
    "# Build 'family' mapping: for each QuestionId in train, take the MC_Answer most frequently labeled as 'True_*'\n",
    "# Used to determine the 'True_/False_' prefix for each test row\n",
    "# -------------------------\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')  # read train set\n",
    "test_df = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')  # read test set\n",
    "\n",
    "train['is_true'] = train['Category'].str.startswith('True')  # Boolean column: whether row belongs to True_* category\n",
    "\n",
    "# Keep only the most frequent correct option as the 'ground truth'\n",
    "correct = (train[train.is_true]\n",
    "           .assign(c=lambda df: df.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count'))\n",
    "           .sort_values('c', ascending=False)\n",
    "           .drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']])\n",
    "correct['is_correct'] = 1  # Mark this (QuestionId, MC_Answer) as the correct answer\n",
    "\n",
    "# Set is_correct=1 for the most common correct option; otherwise 0\n",
    "# Set row_id as index and map to {row_id: 'True_' or 'False_'}\n",
    "fam_map = (test_df.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "                  .assign(is_correct=lambda df: df.is_correct.fillna(0).astype(int))\n",
    "                  .set_index('row_id')['is_correct']\n",
    "                  .map({1: 'True_', 0: 'False_'}).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Ensembling logic\n",
    "# -------------------------\n",
    "def extract_class_probabilities(row, model_suffix='', top_k=25):\n",
    "    \"\"\"Extract class names and probabilities for a given model from a merged row.\n",
    "    \n",
    "    Args:\n",
    "        row: a merged row containing columns from different models\n",
    "        model_suffix: column suffix for the current model (empty for first)\n",
    "        top_k: max number of classes to read (as ordered in classes column)\n",
    "    Returns:\n",
    "        dict: {class_name: prob} for the first top_k classes available for this model\n",
    "    \"\"\"\n",
    "    classes_col = f'top_classes{model_suffix}'  # column name for the class list of current model\n",
    "    if classes_col in row:\n",
    "        classes = row[classes_col].split(' ')[:top_k]  # take the first top_k class labels\n",
    "    else:\n",
    "        return {}  # return empty if column not present\n",
    "    class_probs = {}\n",
    "    for i in range(min(top_k, len(classes))):\n",
    "        prob_col = f'prob_{i}{model_suffix}'  # column name for probability of class i\n",
    "        if prob_col in row:\n",
    "            class_probs[classes[i]] = row[prob_col]  # class name: probability\n",
    "    return class_probs\n",
    "\n",
    "\n",
    "def ensemble_with_disagreement_handling(prob_files, model_weights=None, top_k=3):\n",
    "    \"\"\"Merge multiple probability files for ensembling and apply 'family' prefix filtering.\n",
    "    \n",
    "    Args:\n",
    "        prob_files: list of CSV files from base models; must contain row_id, top_classes, prob_i columns\n",
    "        model_weights: list of weights; length must equal n_models\n",
    "        top_k: take top_k classes after final scoring\n",
    "    Returns:\n",
    "        list[str]: aligned with merged rows; each line contains space-separated \"Category:Misconception\"\n",
    "    \"\"\"\n",
    "    n_models = len(prob_files)  # number of models\n",
    "    prob_dfs = []  # store DataFrames\n",
    "    final_predictions = []  # store final prediction strings\n",
    "\n",
    "    # Read probability outputs of each model\n",
    "    for file_path in prob_files:\n",
    "        df = pd.read_csv(file_path)  \n",
    "        prob_dfs.append(df)\n",
    "    \n",
    "    # Merge by row_id; first keeps columns, later files get suffix _model{i+1}\n",
    "    merged_df = prob_dfs[0]\n",
    "    for i, df in enumerate(prob_dfs[1:], 1):\n",
    "        merged_df = pd.merge(merged_df, df, on='row_id', suffixes=('', f'_model{i+1}'))\n",
    "\n",
    "    # merged_df cols: row_id, top_classes, prob_0, ..., prob_24, top_classes_model2, prob_0_model2, ..., prob_24_model2, top_classes_model3, prob_0_model3, ..., prob_24_model3\n",
    "\n",
    "    for idx, row in merged_df.iterrows():\n",
    "        pref = fam_map[row['row_id']]  # get the 'family' prefix ('True_' or 'False_') for the row\n",
    "        \n",
    "        # Extract class-prob distributions for each model (up to 25 candidates)\n",
    "        all_class_probs = []\n",
    "        for i in range(n_models):\n",
    "            suffix = f'_model{i+1}' if i > 0 else ''  # suffix for all but the first model\n",
    "            class_probs = extract_class_probabilities(row, suffix, top_k=25)\n",
    "            all_class_probs.append(class_probs)\n",
    "        \n",
    "        # Collect union of all class names across models\n",
    "        all_classes = set()\n",
    "        for class_probs in all_class_probs:\n",
    "            all_classes.update(class_probs.keys())\n",
    "        \n",
    "        # Accumulate 'vote count', 'weighted total prob', 'weighted max prob'\n",
    "        # Note: assumes model_weights length equals n_models\n",
    "        class_votes = defaultdict(int)       # times a class is hit across models\n",
    "        class_total_prob = defaultdict(float)  # weighted probability sum\n",
    "        class_max_prob = defaultdict(float)    # largest weighted probability\n",
    "        \n",
    "        for i, class_probs in enumerate(all_class_probs):\n",
    "            weight = model_weights[i]  # current model weight\n",
    "            for class_name, prob in class_probs.items():\n",
    "                class_votes[class_name] += 1 # vote count\n",
    "                class_total_prob[class_name] += prob * weight # weighted total prob\n",
    "                class_max_prob[class_name] = max(class_max_prob[class_name], prob * weight) # weighted max prob\n",
    "        \n",
    "        # Compute final score = weighted_total_prob×0.34 + agreement_ratio×0.33 + weighted_max_prob×0.33\n",
    "        # Intuition: balances overall support, cross-model agreement, and single-model peak confidence\n",
    "        final_scores = {}\n",
    "        for class_name in all_classes:\n",
    "            base_score = class_total_prob[class_name]\n",
    "            agreement_bonus = class_votes[class_name] / n_models\n",
    "            confidence_bonus = class_max_prob[class_name]\n",
    "            final_scores[class_name] = (\n",
    "                base_score * 0.34 +\n",
    "                agreement_bonus * 0.33 +\n",
    "                confidence_bonus * 0.33\n",
    "            )\n",
    "        \n",
    "        # -------------------------\n",
    "        # Family filter: keep only classes with prefix matching pref ('True_'/'False_')\n",
    "        # -------------------------\n",
    "        final_scores = {k: v for k, v in final_scores.items() if k.startswith(pref)}\n",
    "        \n",
    "        # Sort and take top_k classes\n",
    "        sorted_classes = sorted(final_scores.items(), key=lambda x: -x[1])\n",
    "        top_classes = [class_name for class_name, _ in sorted_classes[:top_k]]\n",
    "        \n",
    "        # If fewer than 3 candidates, fill with 'Neither:NA' first; for True_ also add 'Correct:NA'\n",
    "        fillers = [f\"{pref}Neither:NA\"] + ([f\"{pref}Correct:NA\"] if pref == \"True_\" else [])\n",
    "        for f in fillers:\n",
    "            if len(top_classes) >= 3: \n",
    "                # Stop filling once there are already 3 or more\n",
    "                break\n",
    "            if f not in top_classes:\n",
    "                # Add the filler if not already present\n",
    "                top_classes.append(f)\n",
    "\n",
    "        while len(top_classes) < 3:\n",
    "            top_classes.append(fillers[0])  # ensure at least 3\n",
    "        \n",
    "        final_predictions.append(' '.join(top_classes))  # join into a single-line string\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Run ensembling\n",
    "# -------------------------\n",
    "weights = [\n",
    "    1,1,1\n",
    "]\n",
    "\n",
    "prob_files = [\n",
    "    '/kaggle/working/submission_qwen3_4b_prob.csv',\n",
    "    '/kaggle/working/submission_qwen2_8b_prob.csv',\n",
    "    '/kaggle/working/submission_qwen3_14b_prob.csv',\n",
    "]\n",
    "\n",
    "# Run ensemble prediction; here top_k=8 selects the top 8 classes\n",
    "predictions = ensemble_with_disagreement_handling(\n",
    "    prob_files, \n",
    "    model_weights=weights,  \n",
    "    top_k=8\n",
    ")\n",
    "\n",
    "# Build submission file with required column names\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': test_df.row_id.values,\n",
    "    'Category:Misconception': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)  # save submission file\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
